{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720de09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8443fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4e1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 07:30:44,603 - INFO - Loading NLI pipeline with model: cross-encoder/nli-deberta-v3-small\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BSDetector with zero-shot classification pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "2025-05-16 07:30:45,631 - INFO - NLI pipeline loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the capital of France?\n",
      "Answer: Paris\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 07:30:57,913 - INFO - Observed consistency: 0.624\n",
      "2025-05-16 07:30:57,914 - INFO - Self-reflection certainty: 0.500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence: 0.562\n",
      "\n",
      "Question: What is the capital of France?\n",
      "Answer: London\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 07:31:04,799 - INFO - Observed consistency: 0.581\n",
      "2025-05-16 07:31:04,800 - INFO - Self-reflection certainty: 0.750\n",
      "2025-05-16 07:31:04,801 - INFO - Generating 2 candidate answers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence: 0.665\n",
      "\n",
      "Question: What is the distance from Earth to the Moon in kilometers?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 07:31:05,107 - INFO - Candidate 1: The distance from Earth to the Moon (the lunar orbit) is approximately 384,400 kilometers or about 2...\n",
      "2025-05-16 07:31:12,230 - INFO - Observed consistency: 0.589\n",
      "2025-05-16 07:31:12,232 - INFO - Self-reflection certainty: 0.000\n",
      "2025-05-16 07:31:12,233 - INFO - Confidence for candidate 1: 0.295\n",
      "2025-05-16 07:31:12,659 - INFO - Candidate 2: The distance from Earth to the Moon is approximately 384,400 kilometers.\n",
      "\n",
      "This value can vary slight...\n",
      "2025-05-16 07:31:19,221 - INFO - Observed consistency: 0.644\n",
      "2025-05-16 07:31:19,223 - INFO - Self-reflection certainty: 0.750\n",
      "2025-05-16 07:31:19,223 - INFO - Confidence for candidate 2: 0.697\n",
      "2025-05-16 07:31:19,224 - INFO - Selected best answer with confidence: 0.697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best answer: The distance from Earth to the Moon is approximately 384,400 kilometers.\n",
      "\n",
      "This value can vary slightly depending on whether we're talking about the actual distance between the two planets or if it's a hypothetical calculation based on current astronomical knowledge.\n",
      "Confidence: 0.697\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "import json\n",
    "import requests\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BSDetector:\n",
    "    \"\"\"\n",
    "    Implementation of BSDetector algorithm using a zero-shot classification pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_name=\"qwen2.5:0.5b\",\n",
    "        use_nli=True,\n",
    "        nli_model_name=\"cross-encoder/nli-deberta-v3-small\",\n",
    "        alpha=0.7,\n",
    "        beta=0.5,\n",
    "        num_samples=5,\n",
    "        temperature=1.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize BSDetector with specified models and parameters.\n",
    "\n",
    "        Args:\n",
    "            llm_name: Ollama model name to use as primary LLM\n",
    "            use_nli: Whether to try using a real NLI model\n",
    "            nli_model_name: HuggingFace model for NLI\n",
    "            alpha: Weight for NLI similarity vs exact match (paper's α)\n",
    "            beta: Weight for observed consistency vs self-reflection (paper's β)\n",
    "            num_samples: Number of samples for observed consistency (paper's k)\n",
    "            temperature: Temperature for sampling in observed consistency\n",
    "        \"\"\"\n",
    "        self.llm_name = llm_name\n",
    "        self.api_base = \"http://localhost:11434/api\"\n",
    "\n",
    "        # Parameters\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.num_samples = num_samples\n",
    "        self.temperature = temperature\n",
    "\n",
    "        logger.info(f\"Loading NLI pipeline with model: {nli_model_name}\")\n",
    "        self.nli_pipeline = pipeline(\"zero-shot-classification\", model=nli_model_name)\n",
    "        logger.info(\"NLI pipeline loaded successfully\")\n",
    "\n",
    "    def _generate_response(self, prompt, temperature=0.0):\n",
    "        try:\n",
    "            r = requests.post(\n",
    "                f\"{self.api_base}/generate\",\n",
    "                json={\n",
    "                    \"model\": self.llm_name,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"stream\": False,\n",
    "                },\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "\n",
    "            text = r.text.splitlines()[0].strip()\n",
    "            try:\n",
    "                data = json.loads(text)\n",
    "            except json.JSONDecodeError:\n",
    "                try:\n",
    "                    data = r.json()\n",
    "                except Exception:\n",
    "                    m = re.search(r'\"response\":\"([^\"]+)\"', r.text)\n",
    "                    return m.group(1) if m else \"\"\n",
    "\n",
    "            return data.get(\"response\", \"\")\n",
    "\n",
    "        except Exception:\n",
    "            logger.exception(\"Error calling Ollama API\")\n",
    "            raise\n",
    "\n",
    "    def _nli_contradiction_score(self, reference: str, generated: str) -> float:\n",
    "        \"\"\"\n",
    "        Use NLI pipeline to calculate contradiction probability between two texts.\n",
    "\n",
    "        Returns:\n",
    "            1 - contradiction_probability (higher means less similar)\n",
    "        \"\"\"\n",
    "\n",
    "        # Define contradiction and entailment as classes\n",
    "        labels = [\"contradiction\", \"entailment\"]\n",
    "\n",
    "        # Check both directions to mitigate positional bias\n",
    "        # Direction 1: reference -> generated\n",
    "        result1 = self.nli_pipeline(\n",
    "            reference, candidate_labels=labels, hypothesis=generated\n",
    "        )\n",
    "\n",
    "        # Direction 2: generated -> reference\n",
    "        result2 = self.nli_pipeline(\n",
    "            generated, candidate_labels=labels, hypothesis=reference\n",
    "        )\n",
    "\n",
    "        # Extract contradiction probabilities\n",
    "        contradiction_idx1 = result1[\"labels\"].index(\"contradiction\")\n",
    "        contradiction_prob1 = result1[\"scores\"][contradiction_idx1]\n",
    "\n",
    "        contradiction_idx2 = result2[\"labels\"].index(\"contradiction\")\n",
    "        contradiction_prob2 = result2[\"scores\"][contradiction_idx2]\n",
    "\n",
    "        # Average the contradiction probabilities from both directions\n",
    "        avg_contradiction_prob = (contradiction_prob1 + contradiction_prob2) / 2\n",
    "\n",
    "        # Return 1 - contradiction for similarity score (higher means more similar)\n",
    "        return 1.0 - avg_contradiction_prob\n",
    "\n",
    "    def _exact_match(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate exact match (indicator function r_i in paper).\"\"\"\n",
    "        return 1.0 if text1.strip() == text2.strip() else 0.0\n",
    "\n",
    "    def _compute_similarity(self, reference: str, generated: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute similarity combining NLI/fallback and exact match.\n",
    "\n",
    "        Formula from paper: o_i = α*s_i + (1-α)*r_i\n",
    "        \"\"\"\n",
    "        sim = self._nli_contradiction_score(reference, generated)\n",
    "        exact_match = self._exact_match(reference, generated)\n",
    "\n",
    "        # Combine with alpha weight (o_i in paper)\n",
    "        # o_i = α*s_i + (1-α)*r_i\n",
    "        similarity = self.alpha * sim + (1 - self.alpha) * exact_match\n",
    "\n",
    "        return similarity\n",
    "\n",
    "    def observed_consistency(self, question: str, reference_answer: str) -> float:\n",
    "        \"\"\"\n",
    "        Measure observed consistency by generating multiple answers.\n",
    "\n",
    "        Paper formula: O = (1/k) * Σo_i\n",
    "        \"\"\"\n",
    "        # Template for Chain-of-Thought prompting\n",
    "        cot_template = f\"\"\"Please strictly use the following template to provide answer:\n",
    "explanation: [insert step-by-step analysis], answer: [provide your answer]\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "        similarities = []\n",
    "        for i in range(self.num_samples):\n",
    "            try:\n",
    "                # Generate response with temperature sampling\n",
    "                response = self._generate_response(\n",
    "                    cot_template, temperature=self.temperature\n",
    "                )\n",
    "                print(f\"answer {i}: {response}\")\n",
    "\n",
    "                # Compute similarity (o_i in paper)\n",
    "                similarity = self._compute_similarity(reference_answer, response)\n",
    "                similarities.append(similarity)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in observed consistency: {e}\")\n",
    "                # In case of failure, add a neutral value that won't skew the results\n",
    "                similarities.append(0.5)\n",
    "\n",
    "        # Return average similarity as observed consistency (O in paper)\n",
    "        # O = (1/k) * Σo_i\n",
    "        return np.mean(similarities) if similarities else 0.5\n",
    "\n",
    "    def self_reflection_certainty(self, question: str, reference_answer: str) -> float:\n",
    "        \"\"\"\n",
    "        Measure self-reflection certainty by asking the model to evaluate its answer.\n",
    "\n",
    "        Paper formula: S = (score_1 + score_2 + ... + score_n) / n\n",
    "        \"\"\"\n",
    "        # First reflection prompt\n",
    "        reflection_prompt1 = f\"\"\"Question: {question}, Proposed Answer: {reference_answer}\n",
    "Is the proposed answer: (A) Correct (B) Incorrect (C) I am not sure.\n",
    "The output should strictly use the following template:\n",
    "explanation: [insert analysis], answer: [choose one letter from among choices A through C]\"\"\"\n",
    "\n",
    "        # Second reflection prompt\n",
    "        reflection_prompt2 = f\"\"\"Question: {question}, Proposed Answer: {reference_answer}\n",
    "Are you really sure the proposed answer is correct?\n",
    "Choose again: (A) Correct (B) Incorrect (C) I am not sure.\n",
    "The output should strictly use the following template:\n",
    "explanation: [insert analysis], answer: [choose one letter from among choices A through C]\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Get responses\n",
    "            reflection1 = self._generate_response(reflection_prompt1)\n",
    "            reflection2 = self._generate_response(reflection_prompt2)\n",
    "\n",
    "            # Extract answers (simplified parsing)\n",
    "            def extract_letter(reflection):\n",
    "                if \"answer:\" in reflection.lower():\n",
    "                    letter = reflection.lower().split(\"answer:\")[1].strip()[0]\n",
    "                    if letter in \"abc\":\n",
    "                        return letter\n",
    "                # Fallback to searching for the letter\n",
    "                for letter in \"abc\":\n",
    "                    if f\"({letter.upper()})\" in reflection:\n",
    "                        return letter\n",
    "                return \"c\"  # Default to \"not sure\" if parsing fails\n",
    "\n",
    "            letter1 = extract_letter(reflection1)\n",
    "            letter2 = extract_letter(reflection2)\n",
    "\n",
    "            # Convert to numerical values: A=1.0, B=0.0, C=0.5\n",
    "            values = {\"a\": 1.0, \"b\": 0.0, \"c\": 0.5}\n",
    "            score1 = values.get(letter1, 0.5)\n",
    "            score2 = values.get(letter2, 0.5)\n",
    "\n",
    "            # Return average score (S in paper)\n",
    "            # S = (score_1 + score_2) / 2\n",
    "            return (score1 + score2) / 2\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in self-reflection: {e}\")\n",
    "            return 0.5  # Neutral value in case of failure\n",
    "\n",
    "    def compute_confidence(\n",
    "        self, question: str, answer: Optional[str] = None\n",
    "    ) -> Tuple[float, str]:\n",
    "        \"\"\"\n",
    "        Compute overall confidence score for an answer.\n",
    "\n",
    "        Paper formula: C = β*O + (1-β)*S\n",
    "        \"\"\"\n",
    "        # If no answer provided, generate one with temperature=0\n",
    "        if answer is None:\n",
    "            answer = self._generate_response(question)\n",
    "            logger.info(f\"Generated answer: {answer}\")\n",
    "\n",
    "        # Compute the two components\n",
    "        consistency = self.observed_consistency(question, answer)\n",
    "        reflection = self.self_reflection_certainty(question, answer)\n",
    "\n",
    "        logger.info(f\"Observed consistency: {consistency:.3f}\")\n",
    "        logger.info(f\"Self-reflection certainty: {reflection:.3f}\")\n",
    "\n",
    "        # Combine with beta weighting (C in paper)\n",
    "        # C = β*O + (1-β)*S\n",
    "        confidence = self.beta * consistency + (1 - self.beta) * reflection\n",
    "\n",
    "        return confidence, answer\n",
    "\n",
    "    def select_best_answer(\n",
    "        self, question: str, num_candidates: int = 3\n",
    "    ) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Generate multiple candidate answers and select the one with highest confidence.\n",
    "        \"\"\"\n",
    "        candidates = []\n",
    "\n",
    "        # Generate multiple candidate answers with temperature sampling\n",
    "        logger.info(f\"Generating {num_candidates} candidate answers...\")\n",
    "        for i in range(num_candidates):\n",
    "            try:\n",
    "                candidate = self._generate_response(\n",
    "                    question, temperature=self.temperature\n",
    "                )\n",
    "                logger.info(f\"Candidate {i + 1}: {candidate[:100]}...\")\n",
    "\n",
    "                confidence, _ = self.compute_confidence(question, candidate)\n",
    "                candidates.append((candidate, confidence))\n",
    "\n",
    "                logger.info(f\"Confidence for candidate {i + 1}: {confidence:.3f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating candidate {i + 1}: {e}\")\n",
    "\n",
    "        if not candidates:\n",
    "            # If all candidates failed, return a default response\n",
    "            default_answer = \"I'm unable to provide a confident answer at this time.\"\n",
    "            return default_answer, 0.0\n",
    "\n",
    "        # Select the answer with highest confidence\n",
    "        best_candidate = max(candidates, key=lambda x: x[1])\n",
    "\n",
    "        logger.info(f\"Selected best answer with confidence: {best_candidate[1]:.3f}\")\n",
    "        return best_candidate\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing BSDetector with zero-shot classification pipeline...\")\n",
    "\n",
    "    # Initialize with zero-shot classification pipeline\n",
    "    detector = BSDetector(\n",
    "        llm_name=\"qwen2.5:0.5b\",\n",
    "        use_nli=True,\n",
    "        alpha=0.7,\n",
    "        beta=0.5,\n",
    "        num_samples=5,\n",
    "        temperature=0.88888888,\n",
    "    )\n",
    "\n",
    "    # Example 1: Correct answer\n",
    "    question1 = \"What is the capital of France?\"\n",
    "    answer1 = \"Paris\"\n",
    "\n",
    "    print(f\"\\nQuestion: {question1}\")\n",
    "    print(f\"Answer: {answer1}\")\n",
    "    confidence1, _ = detector.compute_confidence(question1, answer1)\n",
    "    print(f\"Confidence: {confidence1:.3f}\")\n",
    "\n",
    "    # Example 2: Wrong answer\n",
    "    question2 = \"What is the capital of France?\"\n",
    "    answer2 = \"London\"\n",
    "\n",
    "    print(f\"\\nQuestion: {question2}\")\n",
    "    print(f\"Answer: {answer2}\")\n",
    "    confidence2, _ = detector.compute_confidence(question2, answer2)\n",
    "    print(f\"Confidence: {confidence2:.3f}\")\n",
    "\n",
    "    # Example 3: Select best answer from multiple candidates\n",
    "    question3 = \"What is the distance from Earth to the Moon in kilometers?\"\n",
    "\n",
    "    print(f\"\\nQuestion: {question3}\")\n",
    "    best_answer, confidence3 = detector.select_best_answer(question3, num_candidates=2)\n",
    "    print(f\"Best answer: {best_answer}\")\n",
    "    print(f\"Confidence: {confidence3:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO NLI implementation wrong, we don't want to compare the prompt to the answer, rather compare differnt prompts (refer to paper)\n",
    "# TODO next action, print candidates of COT, print results of each self reflection, print results of NLI\n",
    "# TODO second action: bettere understand confidence score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45f7be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
