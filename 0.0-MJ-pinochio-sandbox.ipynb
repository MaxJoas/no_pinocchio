{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3720de09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8443fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO NLI implementation wrong, we don't want to compare the prompt to the answer, rather compare differnt prompts (refer to paper)\n",
    "# TODO next action, print candidates of COT, print results of each self reflection, print results of NLI\n",
    "# TODO second action: bettere understand confidence score calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1a556",
   "metadata": {},
   "source": [
    "## Connect to LLM and get example question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3deb7138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest lake in California is **Salton Sea**, located in the southern part of the state.\n",
      "\n",
      "- **Surface Area:** Approximately 343 square miles (though it fluctuates due to evaporation and water inflow).\n",
      "- **Formation:** It was created accidentally in 1905 when the Colorado River breached an irrigation canal and flooded the Salton Basin.\n",
      "- **Salinity:** The lake is saltier than the ocean due to high evaporation rates and agricultural runoff.\n",
      "\n",
      "While **Lake Tahoe** is more famous and deeper, the Salton Sea holds the title for the largest lake by surface area in California. However, it faces significant environmental challenges, including shrinking water levels and ecological concerns.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "model = \"mistral-medium-latest\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the largest lake in California?\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "answer = chat_response.choices[0].message.content\n",
    "print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3450814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template\n",
    "\n",
    "PROMPT_SELF_REFLECTION = Template(\"\"\"\n",
    "1. Question: {{ question }}, Proposed Answer: {{ answer }}\n",
    "Is the proposed answer: (A) Correct (B) Incorrect (C) I am not sure.\n",
    "The output should strictly use the following template:\n",
    "explanation: [insert analysis], answer: [choose one letter from among choices A through C]\n",
    "\n",
    "2. Question: {{ question }}, Proposed Answer: {{ answer }}\n",
    "Are you really sure the proposed answer is correct?\n",
    "Choose again: (A) Correct (B) Incorrect (C) I am not sure.\n",
    "The output should strictly use the following template:\n",
    "explanation: [insert analysis], answer: [choose one letter from among choices A through C]\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c4a42ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_CONSITENCY = Template(\"\"\"\n",
    "Please strictly use the following template to provide answer:\n",
    "explanation: [insert step-by-step analysis], answer: [provide\n",
    "your answer] + Question: {{ question }}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3648b7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_self_reflection_prompt(question, answer):\n",
    "    return PROMPT_SELF_REFLECTION.render(question=question, answer=answer)\n",
    "\n",
    "\n",
    "def get_consistency_prompt(question, answer):\n",
    "    return PROMPT_CONSITENCY.replace(\"[User Provided]\", question).replace(\n",
    "        \"[provide your answer]\", answer\n",
    "    )\n",
    "\n",
    "\n",
    "def get_initial_answer(\n",
    "    question, temperature=0.0, client=\"mistral\", model=\"mistral-medium-latest\"\n",
    "):\n",
    "    if client == \"mistral\":\n",
    "        return get_mistral_answer(question, temperature, model)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Client {client} is not implemented.\")\n",
    "\n",
    "\n",
    "def get_mistral_answer(question, temperature=0.0, model=\"mistral-medium-latest\"):\n",
    "    client = Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n",
    "\n",
    "\n",
    "def get_k_responses(\n",
    "    question, k=5, temperature=1.0, client=\"mistral\", model=\"mistral-medium-latest\"\n",
    "):\n",
    "    responses = []\n",
    "    for _ in range(k):\n",
    "        response = get_initial_answer(question, temperature)\n",
    "        responses.append(response)\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356a406",
   "metadata": {},
   "source": [
    "### Getting outputs for score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3062bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = \"How many survivors were there on the Titanic?\"\n",
    "answer = get_initial_answer(test_question)\n",
    "consistency_responses = get_k_responses(question=test_question, k=5, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53457f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Initial answer: {answer}\")\n",
    "for i, response in enumerate(consistency_responses):\n",
    "    print(\"------------------------\")\n",
    "    print(f\"Response {i + 1}: {response}\")\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70954ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial answer: The exact number of survivors from the sinking of the RMS Titanic on April 15, 1912, is approximately **706** out of the estimated **2,224** passengers and crew on board.\n",
      "\n",
      "Hereâ€™s a breakdown:\n",
      "- **Total on board:** ~2,224 (1,317 passengers + 907 crew)\n",
      "- **Survivors:** ~706 (492 passengers + 214 crew)\n",
      "- **Fatalities:** ~1,517\n",
      "\n",
      "The survival rate was heavily influenced by factors such as class, gender, and age, with women and children in first and second class having higher survival rates. The lifeboats, which could accommodate about 1,178 people, were not filled to capacity, leading to many unnecessary deaths.\n",
      "\n",
      "Would you like more details on specific groups (e.g., by class or gender)?\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initial answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5617b9",
   "metadata": {},
   "source": [
    "### Calc scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7b59575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def get_self_reflection_score(\n",
    "    question, answer, client=\"mistral\", model=\"mistral-medium-latest\"\n",
    "):\n",
    "    prompt = get_self_reflection_prompt(question, answer)\n",
    "    if client == \"mistral\":\n",
    "        response = get_mistral_answer(prompt, temperature=0.0, model=model)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Client {client} is not implemented.\")\n",
    "\n",
    "    print(\"\\nSelf-Reflection Response:\\n\", response)\n",
    "\n",
    "    # Extract \"answer: X\" from each question (expecting two)\n",
    "    def extract_answers(text):\n",
    "        return re.findall(r\"answer:\\s*([ABC])\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    def map_to_score(letter):\n",
    "        return {\"A\": 1.0, \"B\": 0.0, \"C\": 0.5}.get(letter.upper(), None)\n",
    "\n",
    "    answers = extract_answers(response)\n",
    "    scores = [map_to_score(a) for a in answers if map_to_score(a) is not None]\n",
    "\n",
    "    if scores:\n",
    "        return sum(scores) / len(scores)\n",
    "    else:\n",
    "        return None  # Could not parse answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86bbf646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Self-Reflection Response:\n",
      " 1. **First Question:**\n",
      "explanation: The proposed answer provides a well-researched and accurate breakdown of the Titanic's survivors, fatalities, and total passengers/crew. The numbers align with historical records, and the additional context about survival factors (class, gender, lifeboat protocols) is correct and relevant. The offer to provide more details is also appropriate for further inquiry.\n",
      "answer: A\n",
      "\n",
      "2. **Second Question:**\n",
      "explanation: The proposed answer remains consistent with verified historical data. The numbers (706 survivors, 2,224 total on board) are widely accepted by historians, and the explanation of survival disparities by class and gender is accurate. There is no reason to doubt the correctness of this answer.\n",
      "answer: A\n"
     ]
    }
   ],
   "source": [
    "score = get_self_reflection_score(test_question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f16a4d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd80c24",
   "metadata": {},
   "source": [
    "### Next Action Observed Consistency Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc489364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
